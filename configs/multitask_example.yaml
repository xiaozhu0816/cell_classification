# Multi-Task Learning Configuration (Classification + Time Regression)
# 
# Goal: Train a model to jointly:
#   1. Classify cells as infected/uninfected (binary classification)
#   2. Predict time (regression):
#      - Infected cells: time since infection onset
#      - Uninfected cells: elapsed time from experiment start
#
# This preserves temporal information for BOTH infected and uninfected cells!

experiment_name: multitask_resnet50
seed: 42

# ============ Data Configuration ============
data:
  # Paths to your TIFF stack directories
  infected_dir: "/isilon/datalake/gurcan_rsch/scratch/WSI/zhengjie/DATA/GMU_cell_1023/HBMVEC/Infected_well/no_labels"
  uninfected_dir: "/isilon/datalake/gurcan_rsch/scratch/WSI/zhengjie/DATA/GMU_cell_1023/HBMVEC/Uninfected_well/no_labels"
  
  # Labels
  infected_label: 1
  uninfected_label: 0
  
  # Train/val/test split ratios
  split_ratios: [0.7, 0.1, 0.2]  # 70% train, 10% val, 20% test
  split_seed: 123
  
  # Batch sizes
  batch_size: 128  # Reduce from 256 since multi-task has 2 heads (more memory)
  eval_batch_size_multiplier: 2  # Eval uses 256 batch size
  num_workers: 4
  drop_last: false
  balance_sampler: false  # Set to true if class imbalance is severe
  
  # Data augmentation
  transforms:
    image_size: 512
    random_flip: true
    random_rotation: true
    color_jitter: false
    mean: [0.5, 0.5, 0.5]
    std: [0.25, 0.25, 0.25]
  
  # Frame extraction from TIFF stacks
  frames:
    frames_per_hour: 2.0  # 2 frames per hour in your time-lapse
    
    # For training: use a reasonable time window
    # (You can use full range [0, 48] or restrict it based on your needs)
    infected_window_hours: [1, 48]  # Use all infected frames
    uninfected_window_hours: [0, 48]  # Use all uninfected frames
    
    infected_stride: 1  # Take every frame
    uninfected_stride: 1  # Take every frame
    uninfected_use_all: true

# ============ Multi-Task Configuration ============
multitask:
  # When does infection occur? (in hours from experiment start)!
  infection_onset_hour: 1.0 
  
  # Clamp time predictions to this range (prevents unrealistic values)
  clamp_range: [0.0, 48.0]
  
  # Loss weights (tune these to balance the two tasks)
  # Start with equal weights (1.0, 1.0) and adjust based on validation performance
  classification_weight: 1.0  # Weight for classification loss
  regression_weight: 1.0      # Weight for regression loss
  
  # How regression targets are computed:
  #   - Infected cells: target = max(hours_since_start - infection_onset_hour, 0)
  #     Example: Cell at 10h with onset at 2h → target = 8h (infected 8 hours ago)
  #   
  #   - Uninfected cells: target = hours_since_start
  #     Example: Cell at 10h → target = 10h (10 hours into experiment)
  #
  # This preserves temporal information for BOTH classes!

# ============ Model Configuration ============
model:
  name: resnet50  # resnet18, resnet34, resnet50, resnet101, resnet152
  pretrained: true  # Use ImageNet pretrained weights (strongly recommended!)
  num_classes: 2  # Binary classification: infected (1) vs uninfected (0)
  dropout: 0.2  # Dropout rate in task heads
  train_backbone: true  # Fine-tune the ResNet backbone
  hidden_dim: 256  # Hidden layer size in classification/regression heads
                   # Larger = more capacity but slower
                   # Set to 0 for direct linear projection (simpler)
  
  # NEW: Classification-conditioned regression
  # If true, regression head receives classification logits as additional input
  # This helps regression "know" the predicted class and choose appropriate time reference
  use_cls_conditioning: false  # Set to true to enable (experimental feature)

# ============ Training Configuration ============
training:
  epochs: 100  # Increased from 20 to 30 epochs for better convergence
  amp: true  # Mixed precision training (faster on modern GPUs)
  grad_clip: 1.0  # Gradient clipping (prevents exploding gradients)
  checkpoint_dir: checkpoints

# ============ Optimizer Configuration ============
optimizer:
  lr: 0.0001  # Learning rate
  weight_decay: 0.0001  # L2 regularization

# ============ Scheduler Configuration ============
scheduler:
  t_max: 30  # Should match epochs (changed from 20 to 30)
  eta_min: 0.000001  # Minimum learning rate

# ============ How to Run ============
# 
# python train_multitask.py --config configs/multitask_example.yaml
#
# Expected output during training:
#   Epoch 10/20
#   train - total_loss: 0.3214 | cls_loss: 0.1523 | reg_loss: 0.1691
#   val: total_loss:0.2891 | cls_auc:0.9456 | cls_acc:0.8834 | reg_mae:1.892
#   ✓ New best model! cls_auc=0.9456
#
# Metrics tracked:
#   Classification: cls_auc, cls_accuracy, cls_precision, cls_recall, cls_f1
#   Regression: reg_mae, reg_rmse, reg_mse
#
# Best model saved based on: cls_auc (classification AUC)

# ============ Tuning Tips ============
#
# 1. If classification performance is poor:
#    - Increase classification_weight (e.g., 2.0)
#    - Check class balance (set balance_sampler: true if imbalanced)
#    - Use larger model (resnet101) or increase hidden_dim
#
# 2. If regression performance is poor:
#    - Increase regression_weight (e.g., 2.0)
#    - Verify infection_onset_hour is correct!
#    - Check if clamp_range is appropriate
#    - Increase hidden_dim in model config
#
# 3. If overfitting (train good, val poor):
#    - Increase dropout (e.g., 0.3 or 0.4)
#    - Add more data augmentation (color_jitter: true)
#    - Reduce model size (resnet34) or hidden_dim
#    - Use stronger weight_decay
#
# 4. If underfitting (both train and val poor):
#    - Increase model capacity (resnet101, larger hidden_dim)
#    - Train longer (more epochs)
#    - Reduce dropout
#    - Increase learning rate
#
# 5. If GPU out of memory:
#    - Reduce batch_size (128 → 64 → 32)
#    - Use smaller model (resnet34 or resnet18)
#    - Reduce hidden_dim (256 → 128)
#    - Disable amp: false (slower but uses less memory)

